This file contains all the pieces of code that I had to remove or abandon due to poor logic.
I have kept them here to remind of the mistakes I throughout the process.





My major reason for removing this was that I wanted the test data independent of the neural engine.
An analogy of not leaking the exam paper to my student before conducting the test. Call me dumb idc.

    public void setTestData(List<List<Float>> testData) {
        this.testData = testData;
    }
public float[][] getTrainingData() {
        return trainingData;
    }
    public List<List<Float>> getTestData() {
        return testData;
    }



    public void backpropagation(int trainingDataIndex, float learningRate){
        float[] trainingDataPoint = trainingData[trainingDataIndex];
        //the 0th index of this is the label, this is how we will fill up the initial target value.
        //Filling up the initial target layer using the labels
        for(int x=0;x<OUTPUT_SIZE;++x){
            if (x==trainingDataPoint[0]) neuralNetwork.getNeuron(4,x).setActivation(1);
            else neuralNetwork.getNeuron(4,x).setActivation(0);
        }

        //instead of the above mess use a
        //gets me into the middle layer between the targeted layer and its previous layer
        for(int midLayer=0;midLayer<neuralNetwork.getNeurons().size()-2;++midLayer){
            for(int neuronPreviousLayer=0;neuronPreviousLayer<neuralNetwork.getNeurons().get(midLayer+1).size();++neuronPreviousLayer){
                for (int neuronTargetLayer=0;neuronTargetLayer<neuralNetwork.getNeurons().get(midLayer).size();++neuronTargetLayer){
                    //neuron in the target layer will always have the same no of inputs as the targeted layer
                    float neuronInTargetLayer= neuralNetwork.getNeurons().getLast().get(neuronTargetLayer).getActivation();
                    float neuronInTargetedLayer= neuralNetwork.getNeurons().get(midLayer+1).get(neuronTargetLayer).getActivation();
                    float neuronInPreviousLayer= neuralNetwork.getNeuron(midLayer,neuronPreviousLayer).getActivation();
                    float sigDerivative= (float) (Math.exp(-neuralNetwork.getNeuron(midLayer+1,neuronTargetLayer).getVal())/(1+Math.exp(-neuralNetwork.getNeuron(midLayer+1,neuronTargetLayer).getVal())));
                    float gradient= (neuronInTargetedLayer- neuronInTargetLayer) * neuronInPreviousLayer*sigDerivative*learningRate;
                    neuralNetwork.getWeights().setWeight(midLayer,neuronTargetLayer,neuronPreviousLayer,gradient);
                }
            }

            //now let's empty the target layer and fill it up with new inputs
            if(neuralNetwork.getNeurons().get(midLayer+1).size()> neuralNetwork.getNeurons().getLast().size()){
                //add the extra neurons
                for (int x=0;x< neuralNetwork.getNeurons().getLast().size()- neuralNetwork.getNeurons().get(midLayer+1).size();++x){
                    //adds a new neuron to the target layer
                    neuralNetwork.addNeuron(neuralNetwork.getNeurons().size(),new Neuron());
                }
            }else if(neuralNetwork.getNeurons().get(midLayer+1).size()< neuralNetwork.getNeurons().getLast().size()){
                //remove the extra neurons
                for (int x=0;x< neuralNetwork.getNeurons().get(midLayer+1).size()- neuralNetwork.getNeurons().getLast().size();++x){
                    //removes a neuron from the target layer
                    neuralNetwork.getNeurons().removeLast();
                }
            }

            // over here I should be adding the neurons of the next layer as it goes back..
            for (int x=0;x< neuralNetwork.getNeurons().getLast().size();++x){
                neuralNetwork.setNeuron(neuralNetwork.getNeurons().size()-1,x,0);
            }
        }



    }



public void randomizeWeights() {
        for (List<List<Float>> weight : weights) {
            for (List<Float> floats : weight) {
                floats.replaceAll(w -> (float) Math.random() * 2 - 1);
            }
        }
    }


//    public void addWeightsBetweenLayers(int layer1,int layer2) {
//        if(layer1<layer2 || layer2 >= neurons.size()) return;
//        int midLayer = (layer1+layer2)/2;
//        for(int i=0;i<neurons.get(layer1).size();++i) {
//           for(int j=0;j<neurons.get(layer2).size();++j) {
//               weights.setWeight(midLayer,i,j,0);
//           }
//        }
//
//    }
